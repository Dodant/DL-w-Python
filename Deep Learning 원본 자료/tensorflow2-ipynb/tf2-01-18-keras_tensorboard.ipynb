{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4565,
     "status": "ok",
     "timestamp": 1563840196886,
     "user": {
      "displayName": "조현용",
      "photoUrl": "",
      "userId": "06121582825513881890"
     },
     "user_tz": -540
    },
    "id": "rc4LKOCQI4VW",
    "outputId": "f44d0475-e048-4c6a-d69e-a8b08ef17129"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.0.0-beta1 in /usr/local/lib/python3.6/dist-packages (2.0.0b1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.7.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.1.7)\n",
      "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.14.0.dev2019060501)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.11.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.0.8)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.15.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.2.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.8.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (3.7.1)\n",
      "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.14.0a20190603)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.33.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.16.4)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.12.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-beta1) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-beta1) (41.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (0.15.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.0.0-beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kXrZ43slI41h"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMya4z1hI5tW"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HWgEcYd2I60X"
   },
   "outputs": [],
   "source": [
    "def make_random_data():\n",
    "    x = np.random.uniform(low=-2, high=2, size=200)\n",
    "    y = []\n",
    "    for t in x:\n",
    "        r = np.random.normal(loc=0.0, \n",
    "                             scale=(0.5 + t*t/3), \n",
    "                             size=None)\n",
    "        y.append(r)\n",
    "    return  x, 1.726*x -0.84 + np.array(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 630,
     "status": "ok",
     "timestamp": 1563840205234,
     "user": {
      "displayName": "조현용",
      "photoUrl": "",
      "userId": "06121582825513881890"
     },
     "user_tz": -540
    },
    "id": "u2qRMBOWI7ry",
    "outputId": "20d3e2d3-2a42-4e72-d104-cb69bcaea01e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKMEMhJRDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = make_random_data() \n",
    "\n",
    "plt.plot(x, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jLc6SEj1I877"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "x_train, y_train = x[:150], y[:150]\n",
    "x_test, y_test = x[150:], y[150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 666,
     "status": "ok",
     "timestamp": 1563840220998,
     "user": {
      "displayName": "조현용",
      "photoUrl": "",
      "userId": "06121582825513881890"
     },
     "user_tz": -540
    },
    "id": "eIHxoCPnI-t0",
    "outputId": "6cd42ef2-1139-4532-8794-a8e54cd837f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units=1, input_dim=1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callback_list = [tf.keras.callbacks.TensorBoard(log_dir='logs')]\n",
    "model.compile(optimizer='sgd', loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5470,
     "status": "ok",
     "timestamp": 1563840231334,
     "user": {
      "displayName": "조현용",
      "photoUrl": "",
      "userId": "06121582825513881890"
     },
     "user_tz": -540
    },
    "id": "7SwDqTxgJAxq",
    "outputId": "e56b20df-13bc-4077-8c27-a768e7a287cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/300\n",
      "105/105 [==============================] - 0s 1ms/sample - loss: 2.3422 - val_loss: 1.5838\n",
      "Epoch 2/300\n",
      "105/105 [==============================] - 0s 160us/sample - loss: 2.0567 - val_loss: 1.4533\n",
      "Epoch 3/300\n",
      "105/105 [==============================] - 0s 165us/sample - loss: 1.8276 - val_loss: 1.3426\n",
      "Epoch 4/300\n",
      "105/105 [==============================] - 0s 155us/sample - loss: 1.6345 - val_loss: 1.2504\n",
      "Epoch 5/300\n",
      "105/105 [==============================] - 0s 163us/sample - loss: 1.4743 - val_loss: 1.1755\n",
      "Epoch 6/300\n",
      "105/105 [==============================] - 0s 151us/sample - loss: 1.3419 - val_loss: 1.1209\n",
      "Epoch 7/300\n",
      "105/105 [==============================] - 0s 149us/sample - loss: 1.2499 - val_loss: 1.0770\n",
      "Epoch 8/300\n",
      "105/105 [==============================] - 0s 144us/sample - loss: 1.1679 - val_loss: 1.0383\n",
      "Epoch 9/300\n",
      "105/105 [==============================] - 0s 138us/sample - loss: 1.0989 - val_loss: 1.0054\n",
      "Epoch 10/300\n",
      "105/105 [==============================] - 0s 149us/sample - loss: 1.0386 - val_loss: 0.9789\n",
      "Epoch 11/300\n",
      "105/105 [==============================] - 0s 153us/sample - loss: 0.9902 - val_loss: 0.9592\n",
      "Epoch 12/300\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.9561 - val_loss: 0.9449\n",
      "Epoch 13/300\n",
      "105/105 [==============================] - 0s 129us/sample - loss: 0.9253 - val_loss: 0.9350\n",
      "Epoch 14/300\n",
      "105/105 [==============================] - 0s 137us/sample - loss: 0.8986 - val_loss: 0.9213\n",
      "Epoch 15/300\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 0.8691 - val_loss: 0.9069\n",
      "Epoch 16/300\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.8439 - val_loss: 0.9013\n",
      "Epoch 17/300\n",
      "105/105 [==============================] - 0s 135us/sample - loss: 0.8315 - val_loss: 0.8952\n",
      "Epoch 18/300\n",
      "105/105 [==============================] - 0s 145us/sample - loss: 0.8189 - val_loss: 0.8893\n",
      "Epoch 19/300\n",
      "105/105 [==============================] - 0s 131us/sample - loss: 0.8070 - val_loss: 0.8855\n",
      "Epoch 20/300\n",
      "105/105 [==============================] - 0s 137us/sample - loss: 0.7989 - val_loss: 0.8855\n",
      "Epoch 21/300\n",
      "105/105 [==============================] - 0s 143us/sample - loss: 0.7930 - val_loss: 0.8842\n",
      "Epoch 22/300\n",
      "105/105 [==============================] - 0s 111us/sample - loss: 0.7845 - val_loss: 0.8824\n",
      "Epoch 23/300\n",
      "105/105 [==============================] - 0s 147us/sample - loss: 0.7811 - val_loss: 0.8826\n",
      "Epoch 24/300\n",
      "105/105 [==============================] - 0s 136us/sample - loss: 0.7777 - val_loss: 0.8809\n",
      "Epoch 25/300\n",
      "105/105 [==============================] - 0s 149us/sample - loss: 0.7747 - val_loss: 0.8808\n",
      "Epoch 26/300\n",
      "105/105 [==============================] - 0s 138us/sample - loss: 0.7744 - val_loss: 0.8785\n",
      "Epoch 27/300\n",
      "105/105 [==============================] - 0s 125us/sample - loss: 0.7714 - val_loss: 0.8753\n",
      "Epoch 28/300\n",
      "105/105 [==============================] - 0s 141us/sample - loss: 0.7699 - val_loss: 0.8768\n",
      "Epoch 29/300\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 0.7671 - val_loss: 0.8778\n",
      "Epoch 30/300\n",
      "105/105 [==============================] - 0s 121us/sample - loss: 0.7652 - val_loss: 0.8777\n",
      "Epoch 31/300\n",
      "105/105 [==============================] - 0s 134us/sample - loss: 0.7642 - val_loss: 0.8785\n",
      "Epoch 32/300\n",
      "105/105 [==============================] - 0s 129us/sample - loss: 0.7630 - val_loss: 0.8790\n",
      "Epoch 33/300\n",
      "105/105 [==============================] - 0s 137us/sample - loss: 0.7628 - val_loss: 0.8788\n",
      "Epoch 34/300\n",
      "105/105 [==============================] - 0s 201us/sample - loss: 0.7626 - val_loss: 0.8842\n",
      "Epoch 35/300\n",
      "105/105 [==============================] - 0s 129us/sample - loss: 0.7604 - val_loss: 0.8855\n",
      "Epoch 36/300\n",
      "105/105 [==============================] - 0s 137us/sample - loss: 0.7594 - val_loss: 0.8857\n",
      "Epoch 37/300\n",
      "105/105 [==============================] - 0s 148us/sample - loss: 0.7603 - val_loss: 0.8862\n",
      "Epoch 38/300\n",
      "105/105 [==============================] - 0s 173us/sample - loss: 0.7601 - val_loss: 0.8908\n",
      "Epoch 39/300\n",
      "105/105 [==============================] - 0s 139us/sample - loss: 0.7592 - val_loss: 0.8902\n",
      "Epoch 40/300\n",
      "105/105 [==============================] - 0s 162us/sample - loss: 0.7582 - val_loss: 0.8910\n",
      "Epoch 41/300\n",
      "105/105 [==============================] - 0s 179us/sample - loss: 0.7580 - val_loss: 0.8951\n",
      "Epoch 42/300\n",
      "105/105 [==============================] - 0s 136us/sample - loss: 0.7571 - val_loss: 0.8997\n",
      "Epoch 43/300\n",
      "105/105 [==============================] - 0s 139us/sample - loss: 0.7574 - val_loss: 0.8971\n",
      "Epoch 44/300\n",
      "105/105 [==============================] - 0s 116us/sample - loss: 0.7578 - val_loss: 0.8992\n",
      "Epoch 45/300\n",
      "105/105 [==============================] - 0s 130us/sample - loss: 0.7595 - val_loss: 0.9001\n",
      "Epoch 46/300\n",
      "105/105 [==============================] - 0s 136us/sample - loss: 0.7584 - val_loss: 0.8970\n",
      "Epoch 47/300\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7586 - val_loss: 0.8956\n",
      "Epoch 48/300\n",
      "105/105 [==============================] - 0s 127us/sample - loss: 0.7582 - val_loss: 0.8930\n",
      "Epoch 49/300\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7575 - val_loss: 0.8941\n",
      "Epoch 50/300\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 0.7582 - val_loss: 0.8941\n",
      "Epoch 51/300\n",
      "105/105 [==============================] - 0s 144us/sample - loss: 0.7575 - val_loss: 0.8954\n",
      "Epoch 52/300\n",
      "105/105 [==============================] - 0s 189us/sample - loss: 0.7578 - val_loss: 0.8952\n",
      "Epoch 53/300\n",
      "105/105 [==============================] - 0s 160us/sample - loss: 0.7575 - val_loss: 0.8965\n",
      "Epoch 54/300\n",
      "105/105 [==============================] - 0s 173us/sample - loss: 0.7572 - val_loss: 0.8950\n",
      "Epoch 55/300\n",
      "105/105 [==============================] - 0s 149us/sample - loss: 0.7584 - val_loss: 0.8931\n",
      "Epoch 56/300\n",
      "105/105 [==============================] - 0s 127us/sample - loss: 0.7569 - val_loss: 0.8943\n",
      "Epoch 57/300\n",
      "105/105 [==============================] - 0s 138us/sample - loss: 0.7578 - val_loss: 0.8911\n",
      "Epoch 58/300\n",
      "105/105 [==============================] - 0s 139us/sample - loss: 0.7579 - val_loss: 0.8976\n",
      "Epoch 59/300\n",
      "105/105 [==============================] - 0s 168us/sample - loss: 0.7567 - val_loss: 0.9005\n",
      "Epoch 60/300\n",
      "105/105 [==============================] - 0s 149us/sample - loss: 0.7565 - val_loss: 0.9021\n",
      "Epoch 61/300\n",
      "105/105 [==============================] - 0s 159us/sample - loss: 0.7587 - val_loss: 0.9031\n",
      "Epoch 62/300\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7568 - val_loss: 0.9016\n",
      "Epoch 63/300\n",
      "105/105 [==============================] - 0s 136us/sample - loss: 0.7587 - val_loss: 0.9112\n",
      "Epoch 64/300\n",
      "105/105 [==============================] - 0s 155us/sample - loss: 0.7569 - val_loss: 0.9076\n",
      "Epoch 65/300\n",
      "105/105 [==============================] - 0s 177us/sample - loss: 0.7561 - val_loss: 0.9090\n",
      "Epoch 66/300\n",
      "105/105 [==============================] - 0s 149us/sample - loss: 0.7572 - val_loss: 0.9093\n",
      "Epoch 67/300\n",
      "105/105 [==============================] - 0s 144us/sample - loss: 0.7568 - val_loss: 0.9100\n",
      "Epoch 68/300\n",
      "105/105 [==============================] - 0s 154us/sample - loss: 0.7562 - val_loss: 0.9053\n",
      "Epoch 69/300\n",
      "105/105 [==============================] - 0s 150us/sample - loss: 0.7564 - val_loss: 0.9088\n",
      "Epoch 70/300\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 0.7567 - val_loss: 0.9141\n",
      "Epoch 71/300\n",
      "105/105 [==============================] - 0s 144us/sample - loss: 0.7579 - val_loss: 0.9087\n",
      "Epoch 72/300\n",
      "105/105 [==============================] - 0s 139us/sample - loss: 0.7577 - val_loss: 0.9101\n",
      "Epoch 73/300\n",
      "105/105 [==============================] - 0s 161us/sample - loss: 0.7578 - val_loss: 0.9086\n",
      "Epoch 74/300\n",
      "105/105 [==============================] - 0s 155us/sample - loss: 0.7565 - val_loss: 0.9060\n",
      "Epoch 75/300\n",
      "105/105 [==============================] - 0s 132us/sample - loss: 0.7568 - val_loss: 0.9044\n",
      "Epoch 76/300\n",
      "105/105 [==============================] - 0s 163us/sample - loss: 0.7560 - val_loss: 0.9051\n",
      "Epoch 77/300\n",
      "105/105 [==============================] - 0s 187us/sample - loss: 0.7575 - val_loss: 0.9078\n",
      "Epoch 78/300\n",
      "105/105 [==============================] - 0s 127us/sample - loss: 0.7556 - val_loss: 0.9091\n",
      "Epoch 79/300\n",
      "105/105 [==============================] - 0s 129us/sample - loss: 0.7564 - val_loss: 0.9092\n",
      "Epoch 80/300\n",
      "105/105 [==============================] - 0s 130us/sample - loss: 0.7582 - val_loss: 0.9082\n",
      "Epoch 81/300\n",
      "105/105 [==============================] - 0s 134us/sample - loss: 0.7562 - val_loss: 0.9076\n",
      "Epoch 82/300\n",
      "105/105 [==============================] - 0s 146us/sample - loss: 0.7563 - val_loss: 0.9087\n",
      "Epoch 83/300\n",
      "105/105 [==============================] - 0s 145us/sample - loss: 0.7565 - val_loss: 0.9100\n",
      "Epoch 84/300\n",
      "105/105 [==============================] - 0s 163us/sample - loss: 0.7569 - val_loss: 0.9105\n",
      "Epoch 85/300\n",
      "105/105 [==============================] - 0s 119us/sample - loss: 0.7565 - val_loss: 0.9067\n",
      "Epoch 86/300\n",
      "105/105 [==============================] - 0s 176us/sample - loss: 0.7558 - val_loss: 0.9066\n",
      "Epoch 87/300\n",
      "105/105 [==============================] - 0s 139us/sample - loss: 0.7557 - val_loss: 0.9089\n",
      "Epoch 88/300\n",
      "105/105 [==============================] - 0s 152us/sample - loss: 0.7564 - val_loss: 0.9078\n",
      "Epoch 89/300\n",
      "105/105 [==============================] - 0s 135us/sample - loss: 0.7562 - val_loss: 0.9032\n",
      "Epoch 90/300\n",
      "105/105 [==============================] - 0s 191us/sample - loss: 0.7562 - val_loss: 0.9044\n",
      "Epoch 91/300\n",
      "105/105 [==============================] - 0s 156us/sample - loss: 0.7562 - val_loss: 0.9030\n",
      "Epoch 92/300\n",
      "105/105 [==============================] - 0s 126us/sample - loss: 0.7564 - val_loss: 0.9058\n",
      "Epoch 93/300\n",
      "105/105 [==============================] - 0s 132us/sample - loss: 0.7564 - val_loss: 0.9090\n",
      "Epoch 94/300\n",
      "105/105 [==============================] - 0s 125us/sample - loss: 0.7559 - val_loss: 0.9094\n",
      "Epoch 95/300\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7572 - val_loss: 0.9067\n",
      "Epoch 96/300\n",
      "105/105 [==============================] - 0s 141us/sample - loss: 0.7564 - val_loss: 0.9034\n",
      "Epoch 97/300\n",
      "105/105 [==============================] - 0s 130us/sample - loss: 0.7559 - val_loss: 0.9032\n",
      "Epoch 98/300\n",
      "105/105 [==============================] - 0s 167us/sample - loss: 0.7557 - val_loss: 0.9049\n",
      "Epoch 99/300\n",
      "105/105 [==============================] - 0s 110us/sample - loss: 0.7558 - val_loss: 0.9076\n",
      "Epoch 100/300\n",
      "105/105 [==============================] - 0s 109us/sample - loss: 0.7561 - val_loss: 0.9071\n",
      "Epoch 101/300\n",
      "105/105 [==============================] - 0s 116us/sample - loss: 0.7567 - val_loss: 0.9092\n",
      "Epoch 102/300\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7568 - val_loss: 0.9072\n",
      "Epoch 103/300\n",
      "105/105 [==============================] - 0s 136us/sample - loss: 0.7565 - val_loss: 0.9065\n",
      "Epoch 104/300\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.7561 - val_loss: 0.9071\n",
      "Epoch 105/300\n",
      "105/105 [==============================] - 0s 129us/sample - loss: 0.7568 - val_loss: 0.9055\n",
      "Epoch 106/300\n",
      "105/105 [==============================] - 0s 131us/sample - loss: 0.7565 - val_loss: 0.9043\n",
      "Epoch 107/300\n",
      "105/105 [==============================] - 0s 140us/sample - loss: 0.7576 - val_loss: 0.9053\n",
      "Epoch 108/300\n",
      "105/105 [==============================] - 0s 158us/sample - loss: 0.7557 - val_loss: 0.9063\n",
      "Epoch 109/300\n",
      "105/105 [==============================] - 0s 116us/sample - loss: 0.7562 - val_loss: 0.9047\n",
      "Epoch 110/300\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7560 - val_loss: 0.9059\n",
      "Epoch 111/300\n",
      "105/105 [==============================] - 0s 160us/sample - loss: 0.7565 - val_loss: 0.9042\n",
      "Epoch 112/300\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7568 - val_loss: 0.9031\n",
      "Epoch 113/300\n",
      "105/105 [==============================] - 0s 162us/sample - loss: 0.7565 - val_loss: 0.9055\n",
      "Epoch 114/300\n",
      "105/105 [==============================] - 0s 110us/sample - loss: 0.7563 - val_loss: 0.9104\n",
      "Epoch 115/300\n",
      "105/105 [==============================] - 0s 111us/sample - loss: 0.7572 - val_loss: 0.9074\n",
      "Epoch 116/300\n",
      "105/105 [==============================] - 0s 111us/sample - loss: 0.7574 - val_loss: 0.9081\n",
      "Epoch 117/300\n",
      "105/105 [==============================] - 0s 132us/sample - loss: 0.7571 - val_loss: 0.9119\n",
      "Epoch 118/300\n",
      "105/105 [==============================] - 0s 128us/sample - loss: 0.7568 - val_loss: 0.9050\n",
      "Epoch 119/300\n",
      "105/105 [==============================] - 0s 128us/sample - loss: 0.7561 - val_loss: 0.9051\n",
      "Epoch 120/300\n",
      "105/105 [==============================] - 0s 140us/sample - loss: 0.7559 - val_loss: 0.9057\n",
      "Epoch 121/300\n",
      "105/105 [==============================] - 0s 152us/sample - loss: 0.7557 - val_loss: 0.9048\n",
      "Epoch 122/300\n",
      "105/105 [==============================] - 0s 128us/sample - loss: 0.7566 - val_loss: 0.9067\n",
      "Epoch 123/300\n",
      "105/105 [==============================] - 0s 137us/sample - loss: 0.7571 - val_loss: 0.9049\n",
      "Epoch 124/300\n",
      "105/105 [==============================] - 0s 129us/sample - loss: 0.7564 - val_loss: 0.8984\n",
      "Epoch 125/300\n",
      "105/105 [==============================] - 0s 148us/sample - loss: 0.7566 - val_loss: 0.8982\n",
      "Epoch 126/300\n",
      "105/105 [==============================] - 0s 192us/sample - loss: 0.7568 - val_loss: 0.8969\n",
      "Epoch 127/300\n",
      "105/105 [==============================] - 0s 108us/sample - loss: 0.7566 - val_loss: 0.8995\n",
      "Epoch 128/300\n",
      "105/105 [==============================] - 0s 115us/sample - loss: 0.7563 - val_loss: 0.9010\n",
      "Epoch 129/300\n",
      "105/105 [==============================] - 0s 101us/sample - loss: 0.7561 - val_loss: 0.9020\n",
      "Epoch 130/300\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7573 - val_loss: 0.8989\n",
      "Epoch 131/300\n",
      "105/105 [==============================] - 0s 141us/sample - loss: 0.7571 - val_loss: 0.8985\n",
      "Epoch 132/300\n",
      "105/105 [==============================] - 0s 112us/sample - loss: 0.7565 - val_loss: 0.8977\n",
      "Epoch 133/300\n",
      "105/105 [==============================] - 0s 134us/sample - loss: 0.7590 - val_loss: 0.8992\n",
      "Epoch 134/300\n",
      "105/105 [==============================] - 0s 101us/sample - loss: 0.7560 - val_loss: 0.8973\n",
      "Epoch 135/300\n",
      "105/105 [==============================] - 0s 122us/sample - loss: 0.7591 - val_loss: 0.8977\n",
      "Epoch 136/300\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7564 - val_loss: 0.8948\n",
      "Epoch 137/300\n",
      "105/105 [==============================] - 0s 111us/sample - loss: 0.7576 - val_loss: 0.8928\n",
      "Epoch 138/300\n",
      "105/105 [==============================] - 0s 120us/sample - loss: 0.7566 - val_loss: 0.8937\n",
      "Epoch 139/300\n",
      "105/105 [==============================] - 0s 108us/sample - loss: 0.7565 - val_loss: 0.8950\n",
      "Epoch 140/300\n",
      "105/105 [==============================] - 0s 105us/sample - loss: 0.7575 - val_loss: 0.8943\n",
      "Epoch 141/300\n",
      "105/105 [==============================] - 0s 108us/sample - loss: 0.7563 - val_loss: 0.8955\n",
      "Epoch 142/300\n",
      "105/105 [==============================] - 0s 110us/sample - loss: 0.7573 - val_loss: 0.9003\n",
      "Epoch 143/300\n",
      "105/105 [==============================] - 0s 115us/sample - loss: 0.7574 - val_loss: 0.8975\n",
      "Epoch 144/300\n",
      "105/105 [==============================] - 0s 127us/sample - loss: 0.7581 - val_loss: 0.8974\n",
      "Epoch 145/300\n",
      "105/105 [==============================] - 0s 112us/sample - loss: 0.7563 - val_loss: 0.8981\n",
      "Epoch 146/300\n",
      "105/105 [==============================] - 0s 154us/sample - loss: 0.7564 - val_loss: 0.8967\n",
      "Epoch 147/300\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7571 - val_loss: 0.9021\n",
      "Epoch 148/300\n",
      "105/105 [==============================] - 0s 111us/sample - loss: 0.7566 - val_loss: 0.9006\n",
      "Epoch 149/300\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7566 - val_loss: 0.8991\n",
      "Epoch 150/300\n",
      "105/105 [==============================] - 0s 145us/sample - loss: 0.7563 - val_loss: 0.8978\n",
      "Epoch 151/300\n",
      "105/105 [==============================] - 0s 132us/sample - loss: 0.7564 - val_loss: 0.9046\n",
      "Epoch 152/300\n",
      "105/105 [==============================] - 0s 200us/sample - loss: 0.7566 - val_loss: 0.9039\n",
      "Epoch 153/300\n",
      "105/105 [==============================] - 0s 110us/sample - loss: 0.7567 - val_loss: 0.9020\n",
      "Epoch 154/300\n",
      "105/105 [==============================] - 0s 109us/sample - loss: 0.7559 - val_loss: 0.9024\n",
      "Epoch 155/300\n",
      "105/105 [==============================] - 0s 110us/sample - loss: 0.7559 - val_loss: 0.9004\n",
      "Epoch 156/300\n",
      "105/105 [==============================] - 0s 121us/sample - loss: 0.7560 - val_loss: 0.8999\n",
      "Epoch 157/300\n",
      "105/105 [==============================] - 0s 113us/sample - loss: 0.7566 - val_loss: 0.8999\n",
      "Epoch 158/300\n",
      "105/105 [==============================] - 0s 118us/sample - loss: 0.7561 - val_loss: 0.9007\n",
      "Epoch 159/300\n",
      "105/105 [==============================] - 0s 118us/sample - loss: 0.7566 - val_loss: 0.8994\n",
      "Epoch 160/300\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.7561 - val_loss: 0.8960\n",
      "Epoch 161/300\n",
      "105/105 [==============================] - 0s 110us/sample - loss: 0.7567 - val_loss: 0.8995\n",
      "Epoch 162/300\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.7582 - val_loss: 0.9010\n",
      "Epoch 163/300\n",
      "105/105 [==============================] - 0s 119us/sample - loss: 0.7571 - val_loss: 0.8996\n",
      "Epoch 164/300\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 0.7573 - val_loss: 0.8959\n",
      "Epoch 165/300\n",
      "105/105 [==============================] - 0s 146us/sample - loss: 0.7573 - val_loss: 0.8992\n",
      "Epoch 166/300\n",
      "105/105 [==============================] - 0s 163us/sample - loss: 0.7566 - val_loss: 0.8988\n",
      "Epoch 167/300\n",
      "105/105 [==============================] - 0s 122us/sample - loss: 0.7604 - val_loss: 0.8964\n",
      "Epoch 168/300\n",
      "105/105 [==============================] - 0s 119us/sample - loss: 0.7569 - val_loss: 0.8997\n",
      "Epoch 169/300\n",
      "105/105 [==============================] - 0s 111us/sample - loss: 0.7580 - val_loss: 0.9000\n",
      "Epoch 170/300\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7579 - val_loss: 0.9007\n",
      "Epoch 171/300\n",
      "105/105 [==============================] - 0s 110us/sample - loss: 0.7559 - val_loss: 0.8999\n",
      "Epoch 172/300\n",
      "105/105 [==============================] - 0s 107us/sample - loss: 0.7560 - val_loss: 0.8995\n",
      "Epoch 173/300\n",
      "105/105 [==============================] - 0s 113us/sample - loss: 0.7558 - val_loss: 0.8987\n",
      "Epoch 174/300\n",
      "105/105 [==============================] - 0s 116us/sample - loss: 0.7561 - val_loss: 0.8985\n",
      "Epoch 175/300\n",
      "105/105 [==============================] - 0s 126us/sample - loss: 0.7563 - val_loss: 0.8982\n",
      "Epoch 176/300\n",
      "105/105 [==============================] - 0s 138us/sample - loss: 0.7563 - val_loss: 0.8978\n",
      "Epoch 177/300\n",
      "105/105 [==============================] - 0s 120us/sample - loss: 0.7563 - val_loss: 0.9034\n",
      "Epoch 178/300\n",
      "105/105 [==============================] - 0s 136us/sample - loss: 0.7563 - val_loss: 0.9036\n",
      "Epoch 179/300\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.7562 - val_loss: 0.9026\n",
      "Epoch 180/300\n",
      "105/105 [==============================] - 0s 113us/sample - loss: 0.7560 - val_loss: 0.9041\n",
      "Epoch 181/300\n",
      "105/105 [==============================] - 0s 150us/sample - loss: 0.7564 - val_loss: 0.9036\n",
      "Epoch 182/300\n",
      "105/105 [==============================] - 0s 128us/sample - loss: 0.7564 - val_loss: 0.9010\n",
      "Epoch 183/300\n",
      "105/105 [==============================] - 0s 128us/sample - loss: 0.7559 - val_loss: 0.9029\n",
      "Epoch 184/300\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7563 - val_loss: 0.9075\n",
      "Epoch 185/300\n",
      "105/105 [==============================] - 0s 118us/sample - loss: 0.7559 - val_loss: 0.9067\n",
      "Epoch 186/300\n",
      "105/105 [==============================] - 0s 134us/sample - loss: 0.7578 - val_loss: 0.9054\n",
      "Epoch 187/300\n",
      "105/105 [==============================] - 0s 112us/sample - loss: 0.7569 - val_loss: 0.9000\n",
      "Epoch 188/300\n",
      "105/105 [==============================] - 0s 110us/sample - loss: 0.7564 - val_loss: 0.8987\n",
      "Epoch 189/300\n",
      "105/105 [==============================] - 0s 109us/sample - loss: 0.7563 - val_loss: 0.8964\n",
      "Epoch 190/300\n",
      "105/105 [==============================] - 0s 113us/sample - loss: 0.7563 - val_loss: 0.8987\n",
      "Epoch 191/300\n",
      "105/105 [==============================] - 0s 121us/sample - loss: 0.7561 - val_loss: 0.9019\n",
      "Epoch 192/300\n",
      "105/105 [==============================] - 0s 108us/sample - loss: 0.7562 - val_loss: 0.9048\n",
      "Epoch 193/300\n",
      "105/105 [==============================] - 0s 120us/sample - loss: 0.7573 - val_loss: 0.9029\n",
      "Epoch 194/300\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7562 - val_loss: 0.9036\n",
      "Epoch 195/300\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7575 - val_loss: 0.9022\n",
      "Epoch 196/300\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7571 - val_loss: 0.9021\n",
      "Epoch 197/300\n",
      "105/105 [==============================] - 0s 136us/sample - loss: 0.7569 - val_loss: 0.8979\n",
      "Epoch 198/300\n",
      "105/105 [==============================] - 0s 100us/sample - loss: 0.7568 - val_loss: 0.8986\n",
      "Epoch 199/300\n",
      "105/105 [==============================] - 0s 119us/sample - loss: 0.7565 - val_loss: 0.8953\n",
      "Epoch 200/300\n",
      "105/105 [==============================] - 0s 126us/sample - loss: 0.7585 - val_loss: 0.8963\n",
      "Epoch 201/300\n",
      "105/105 [==============================] - 0s 115us/sample - loss: 0.7564 - val_loss: 0.8927\n",
      "Epoch 202/300\n",
      "105/105 [==============================] - 0s 120us/sample - loss: 0.7567 - val_loss: 0.8938\n",
      "Epoch 203/300\n",
      "105/105 [==============================] - 0s 109us/sample - loss: 0.7570 - val_loss: 0.8951\n",
      "Epoch 204/300\n",
      "105/105 [==============================] - 0s 118us/sample - loss: 0.7576 - val_loss: 0.8920\n",
      "Epoch 205/300\n",
      "105/105 [==============================] - 0s 125us/sample - loss: 0.7569 - val_loss: 0.8940\n",
      "Epoch 206/300\n",
      "105/105 [==============================] - 0s 121us/sample - loss: 0.7572 - val_loss: 0.8915\n",
      "Epoch 207/300\n",
      "105/105 [==============================] - 0s 146us/sample - loss: 0.7572 - val_loss: 0.8946\n",
      "Epoch 208/300\n",
      "105/105 [==============================] - 0s 117us/sample - loss: 0.7579 - val_loss: 0.8947\n",
      "Epoch 209/300\n",
      "105/105 [==============================] - 0s 120us/sample - loss: 0.7574 - val_loss: 0.8952\n",
      "Epoch 210/300\n",
      "105/105 [==============================] - 0s 107us/sample - loss: 0.7570 - val_loss: 0.8955\n",
      "Epoch 211/300\n",
      "105/105 [==============================] - 0s 127us/sample - loss: 0.7573 - val_loss: 0.9038\n",
      "Epoch 212/300\n",
      "105/105 [==============================] - 0s 112us/sample - loss: 0.7568 - val_loss: 0.9040\n",
      "Epoch 213/300\n",
      "105/105 [==============================] - 0s 126us/sample - loss: 0.7564 - val_loss: 0.9031\n",
      "Epoch 214/300\n",
      "105/105 [==============================] - 0s 105us/sample - loss: 0.7574 - val_loss: 0.9022\n",
      "Epoch 215/300\n",
      "105/105 [==============================] - 0s 125us/sample - loss: 0.7576 - val_loss: 0.9033\n",
      "Epoch 216/300\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7578 - val_loss: 0.9047\n",
      "Epoch 217/300\n",
      "105/105 [==============================] - 0s 117us/sample - loss: 0.7565 - val_loss: 0.9024\n",
      "Epoch 218/300\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7562 - val_loss: 0.9010\n",
      "Epoch 219/300\n",
      "105/105 [==============================] - 0s 119us/sample - loss: 0.7568 - val_loss: 0.9008\n",
      "Epoch 220/300\n",
      "105/105 [==============================] - 0s 135us/sample - loss: 0.7568 - val_loss: 0.8993\n",
      "Epoch 221/300\n",
      "105/105 [==============================] - 0s 112us/sample - loss: 0.7563 - val_loss: 0.8980\n",
      "Epoch 222/300\n",
      "105/105 [==============================] - 0s 131us/sample - loss: 0.7561 - val_loss: 0.8987\n",
      "Epoch 223/300\n",
      "105/105 [==============================] - 0s 127us/sample - loss: 0.7608 - val_loss: 0.8977\n",
      "Epoch 224/300\n",
      "105/105 [==============================] - 0s 112us/sample - loss: 0.7569 - val_loss: 0.8995\n",
      "Epoch 225/300\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.7569 - val_loss: 0.8959\n",
      "Epoch 226/300\n",
      "105/105 [==============================] - 0s 118us/sample - loss: 0.7575 - val_loss: 0.8893\n",
      "Epoch 227/300\n",
      "105/105 [==============================] - 0s 207us/sample - loss: 0.7582 - val_loss: 0.8899\n",
      "Epoch 228/300\n",
      "105/105 [==============================] - 0s 135us/sample - loss: 0.7576 - val_loss: 0.8904\n",
      "Epoch 229/300\n",
      "105/105 [==============================] - 0s 126us/sample - loss: 0.7575 - val_loss: 0.8885\n",
      "Epoch 230/300\n",
      "105/105 [==============================] - 0s 120us/sample - loss: 0.7590 - val_loss: 0.8899\n",
      "Epoch 231/300\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7575 - val_loss: 0.8898\n",
      "Epoch 232/300\n",
      "105/105 [==============================] - 0s 112us/sample - loss: 0.7581 - val_loss: 0.8933\n",
      "Epoch 233/300\n",
      "105/105 [==============================] - 0s 136us/sample - loss: 0.7567 - val_loss: 0.8925\n",
      "Epoch 234/300\n",
      "105/105 [==============================] - 0s 115us/sample - loss: 0.7586 - val_loss: 0.8988\n",
      "Epoch 235/300\n",
      "105/105 [==============================] - 0s 108us/sample - loss: 0.7560 - val_loss: 0.8980\n",
      "Epoch 236/300\n",
      "105/105 [==============================] - 0s 126us/sample - loss: 0.7566 - val_loss: 0.8971\n",
      "Epoch 237/300\n",
      "105/105 [==============================] - 0s 112us/sample - loss: 0.7571 - val_loss: 0.8979\n",
      "Epoch 238/300\n",
      "105/105 [==============================] - 0s 110us/sample - loss: 0.7571 - val_loss: 0.9019\n",
      "Epoch 239/300\n",
      "105/105 [==============================] - 0s 119us/sample - loss: 0.7562 - val_loss: 0.9040\n",
      "Epoch 240/300\n",
      "105/105 [==============================] - 0s 138us/sample - loss: 0.7568 - val_loss: 0.9043\n",
      "Epoch 241/300\n",
      "105/105 [==============================] - 0s 126us/sample - loss: 0.7560 - val_loss: 0.9072\n",
      "Epoch 242/300\n",
      "105/105 [==============================] - 0s 115us/sample - loss: 0.7565 - val_loss: 0.9054\n",
      "Epoch 243/300\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7561 - val_loss: 0.9077\n",
      "Epoch 244/300\n",
      "105/105 [==============================] - 0s 134us/sample - loss: 0.7570 - val_loss: 0.9071\n",
      "Epoch 245/300\n",
      "105/105 [==============================] - 0s 108us/sample - loss: 0.7570 - val_loss: 0.9029\n",
      "Epoch 246/300\n",
      "105/105 [==============================] - 0s 116us/sample - loss: 0.7592 - val_loss: 0.9033\n",
      "Epoch 247/300\n",
      "105/105 [==============================] - 0s 128us/sample - loss: 0.7561 - val_loss: 0.8996\n",
      "Epoch 248/300\n",
      "105/105 [==============================] - 0s 115us/sample - loss: 0.7568 - val_loss: 0.9000\n",
      "Epoch 249/300\n",
      "105/105 [==============================] - 0s 117us/sample - loss: 0.7561 - val_loss: 0.8991\n",
      "Epoch 250/300\n",
      "105/105 [==============================] - 0s 122us/sample - loss: 0.7570 - val_loss: 0.9054\n",
      "Epoch 251/300\n",
      "105/105 [==============================] - 0s 111us/sample - loss: 0.7561 - val_loss: 0.9014\n",
      "Epoch 252/300\n",
      "105/105 [==============================] - 0s 107us/sample - loss: 0.7559 - val_loss: 0.9024\n",
      "Epoch 253/300\n",
      "105/105 [==============================] - 0s 102us/sample - loss: 0.7559 - val_loss: 0.9001\n",
      "Epoch 254/300\n",
      "105/105 [==============================] - 0s 113us/sample - loss: 0.7570 - val_loss: 0.8991\n",
      "Epoch 255/300\n",
      "105/105 [==============================] - 0s 136us/sample - loss: 0.7575 - val_loss: 0.8969\n",
      "Epoch 256/300\n",
      "105/105 [==============================] - 0s 131us/sample - loss: 0.7568 - val_loss: 0.9035\n",
      "Epoch 257/300\n",
      "105/105 [==============================] - 0s 139us/sample - loss: 0.7569 - val_loss: 0.9029\n",
      "Epoch 258/300\n",
      "105/105 [==============================] - 0s 113us/sample - loss: 0.7570 - val_loss: 0.9071\n",
      "Epoch 259/300\n",
      "105/105 [==============================] - 0s 129us/sample - loss: 0.7565 - val_loss: 0.9099\n",
      "Epoch 260/300\n",
      "105/105 [==============================] - 0s 126us/sample - loss: 0.7565 - val_loss: 0.9044\n",
      "Epoch 261/300\n",
      "105/105 [==============================] - 0s 127us/sample - loss: 0.7568 - val_loss: 0.9046\n",
      "Epoch 262/300\n",
      "105/105 [==============================] - 0s 118us/sample - loss: 0.7558 - val_loss: 0.9054\n",
      "Epoch 263/300\n",
      "105/105 [==============================] - 0s 117us/sample - loss: 0.7564 - val_loss: 0.9109\n",
      "Epoch 264/300\n",
      "105/105 [==============================] - 0s 117us/sample - loss: 0.7562 - val_loss: 0.9074\n",
      "Epoch 265/300\n",
      "105/105 [==============================] - 0s 99us/sample - loss: 0.7558 - val_loss: 0.9058\n",
      "Epoch 266/300\n",
      "105/105 [==============================] - 0s 132us/sample - loss: 0.7566 - val_loss: 0.9069\n",
      "Epoch 267/300\n",
      "105/105 [==============================] - 0s 119us/sample - loss: 0.7562 - val_loss: 0.9044\n",
      "Epoch 268/300\n",
      "105/105 [==============================] - 0s 144us/sample - loss: 0.7562 - val_loss: 0.9050\n",
      "Epoch 269/300\n",
      "105/105 [==============================] - 0s 113us/sample - loss: 0.7572 - val_loss: 0.9082\n",
      "Epoch 270/300\n",
      "105/105 [==============================] - 0s 120us/sample - loss: 0.7577 - val_loss: 0.9106\n",
      "Epoch 271/300\n",
      "105/105 [==============================] - 0s 128us/sample - loss: 0.7560 - val_loss: 0.9083\n",
      "Epoch 272/300\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7572 - val_loss: 0.9085\n",
      "Epoch 273/300\n",
      "105/105 [==============================] - 0s 113us/sample - loss: 0.7564 - val_loss: 0.9053\n",
      "Epoch 274/300\n",
      "105/105 [==============================] - 0s 146us/sample - loss: 0.7563 - val_loss: 0.9075\n",
      "Epoch 275/300\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7564 - val_loss: 0.9107\n",
      "Epoch 276/300\n",
      "105/105 [==============================] - 0s 113us/sample - loss: 0.7560 - val_loss: 0.9143\n",
      "Epoch 277/300\n",
      "105/105 [==============================] - 0s 116us/sample - loss: 0.7599 - val_loss: 0.9119\n",
      "Epoch 278/300\n",
      "105/105 [==============================] - 0s 112us/sample - loss: 0.7561 - val_loss: 0.9103\n",
      "Epoch 279/300\n",
      "105/105 [==============================] - 0s 112us/sample - loss: 0.7573 - val_loss: 0.9176\n",
      "Epoch 280/300\n",
      "105/105 [==============================] - 0s 127us/sample - loss: 0.7574 - val_loss: 0.9184\n",
      "Epoch 281/300\n",
      "105/105 [==============================] - 0s 117us/sample - loss: 0.7582 - val_loss: 0.9155\n",
      "Epoch 282/300\n",
      "105/105 [==============================] - 0s 120us/sample - loss: 0.7570 - val_loss: 0.9134\n",
      "Epoch 283/300\n",
      "105/105 [==============================] - 0s 128us/sample - loss: 0.7564 - val_loss: 0.9104\n",
      "Epoch 284/300\n",
      "105/105 [==============================] - 0s 130us/sample - loss: 0.7558 - val_loss: 0.9085\n",
      "Epoch 285/300\n",
      "105/105 [==============================] - 0s 118us/sample - loss: 0.7565 - val_loss: 0.9044\n",
      "Epoch 286/300\n",
      "105/105 [==============================] - 0s 145us/sample - loss: 0.7558 - val_loss: 0.9031\n",
      "Epoch 287/300\n",
      "105/105 [==============================] - 0s 112us/sample - loss: 0.7559 - val_loss: 0.9065\n",
      "Epoch 288/300\n",
      "105/105 [==============================] - 0s 139us/sample - loss: 0.7567 - val_loss: 0.9131\n",
      "Epoch 289/300\n",
      "105/105 [==============================] - 0s 111us/sample - loss: 0.7572 - val_loss: 0.9054\n",
      "Epoch 290/300\n",
      "105/105 [==============================] - 0s 105us/sample - loss: 0.7574 - val_loss: 0.9057\n",
      "Epoch 291/300\n",
      "105/105 [==============================] - 0s 105us/sample - loss: 0.7559 - val_loss: 0.9039\n",
      "Epoch 292/300\n",
      "105/105 [==============================] - 0s 112us/sample - loss: 0.7558 - val_loss: 0.9062\n",
      "Epoch 293/300\n",
      "105/105 [==============================] - 0s 115us/sample - loss: 0.7563 - val_loss: 0.9065\n",
      "Epoch 294/300\n",
      "105/105 [==============================] - 0s 115us/sample - loss: 0.7566 - val_loss: 0.9138\n",
      "Epoch 295/300\n",
      "105/105 [==============================] - 0s 125us/sample - loss: 0.7562 - val_loss: 0.9127\n",
      "Epoch 296/300\n",
      "105/105 [==============================] - 0s 126us/sample - loss: 0.7562 - val_loss: 0.9079\n",
      "Epoch 297/300\n",
      "105/105 [==============================] - 0s 131us/sample - loss: 0.7566 - val_loss: 0.9085\n",
      "Epoch 298/300\n",
      "105/105 [==============================] - 0s 110us/sample - loss: 0.7569 - val_loss: 0.9118\n",
      "Epoch 299/300\n",
      "105/105 [==============================] - 0s 117us/sample - loss: 0.7571 - val_loss: 0.9100\n",
      "Epoch 300/300\n",
      "105/105 [==============================] - 0s 99us/sample - loss: 0.7562 - val_loss: 0.9118\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=300, \n",
    "                    callbacks=callback_list, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 655,
     "status": "ok",
     "timestamp": 1563840238108,
     "user": {
      "displayName": "조현용",
      "photoUrl": "",
      "userId": "06121582825513881890"
     },
     "user_tz": -540
    },
    "id": "joRgj3KwJCID",
    "outputId": "34e5585d-a72c-4bcf-9878-73eee32ca89f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = np.arange(1, 300+1)\n",
    "plt.plot(epochs, history.history['loss'], label='Training loss')\n",
    "plt.plot(epochs, history.history['val_loss'], label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 28296), started 0:00:49 ago. (Use '!kill 28296' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x4dede48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tf2-01-18-keras_tensorboard.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
